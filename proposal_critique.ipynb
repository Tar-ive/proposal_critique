{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17g-nkKKuG6Ygfm2VqHE3EkTORtD4z1r3",
      "authorship_tag": "ABX9TyMHNOwYq+5Ooesj9fC8pK9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tar-ive/proposal_critique/blob/main/proposal_critique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw_ODDrio26-",
        "outputId": "8bbe2b12-8dae-4421-819f-659ef11e6403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCULqluyouEu",
        "outputId": "b8e02c9f-70e1-4390-af9e-6bc9ce76d860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from PDF...\n",
            "Parsing basic information...\n",
            "Parsing personnel information...\n",
            "Parsing budget information...\n",
            "Parsing research information...\n",
            "Parsing SBIR information...\n",
            "Main grant data saved to nih_grant_harty_main.csv\n",
            "Personnel data saved to nih_grant_harty_personnel.csv\n",
            "Summary data saved to nih_grant_harty_summary.csv\n",
            "\n",
            "Extracted Grant Information:\n",
            "  PI_Name Application_ID Funding_Opportunity  \\\n",
            "0   Harty              1           PA-20-265   \n",
            "\n",
            "                                       Project_Title  \\\n",
            "0  Development of Small Molecule Therapeutics Tar...   \n",
            "\n",
            "                              Organization Start_Date    End_Date  \\\n",
            "0  Fox Chase Chemical Diversity Center Inc             04/01/2021   \n",
            "\n",
            "  Total_Federal_Funds Total_Project_Funds  \\\n",
            "0                                           \n",
            "\n",
            "                                            Abstract  \\\n",
            "0  The ultimate goal of this Phase II application...   \n",
            "\n",
            "                                       Specific_Aims Human_Subjects  \\\n",
            "0  The ultimate goal of this Phase II application...                  \n",
            "\n",
            "  Vertebrate_Animals Clinical_Trial Program_Type Application_Type  \\\n",
            "0                                                                   \n",
            "\n",
            "  Small_Business_Eligible  \n",
            "0                          \n",
            "\n",
            "Extracted Personnel Information:\n",
            "                                      First_Name Last_Name  \\\n",
            "0  Olena Middle Name Last Name*: Shtanko Suffix:      Kulp   \n",
            "1                      Allen Middle Name Bernard     Reitz   \n",
            "\n",
            "                                           Full_Name  \\\n",
            "0  Olena Middle Name Last Name*: Shtanko Suffix: ...   \n",
            "1                    Allen Middle Name Bernard Reitz   \n",
            "\n",
            "                                           Role  \\\n",
            "0  Co-Investigator Other Project Role Category:   \n",
            "1  Co-Investigator Other Project Role Category:   \n",
            "\n",
            "                          Organization           Position  \\\n",
            "0  Texas Biomedical Research Institute  Staff Scientist I   \n",
            "1  Fox Chase Chemical Diversity Center                CEO   \n",
            "\n",
            "                  Degree  \n",
            "0  PHD Degree Year: 2010  \n",
            "1  PHD Degree Year: 1982  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import re\n",
        "from typing import Dict, List, Any\n",
        "import io\n",
        "\n",
        "class NIHGrantParser:\n",
        "    def __init__(self):\n",
        "        self.data = {}\n",
        "\n",
        "    def extract_basic_info(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract basic application information\"\"\"\n",
        "        basic_info = {}\n",
        "\n",
        "        # Extract PI name\n",
        "        pi_match = re.search(r'Contact PD/PI:\\s*([^,\\n]+)', text)\n",
        "        basic_info['PI_Name'] = pi_match.group(1).strip() if pi_match else ''\n",
        "\n",
        "        # Extract application ID\n",
        "        app_id_match = re.search(r'Application Identifier\\s*([A-Z0-9]+)', text)\n",
        "        basic_info['Application_ID'] = app_id_match.group(1) if app_id_match else ''\n",
        "\n",
        "        # Extract funding opportunity\n",
        "        funding_match = re.search(r'Funding Opportunity Number:\\s*([A-Z0-9-]+)', text)\n",
        "        basic_info['Funding_Opportunity'] = funding_match.group(1) if funding_match else ''\n",
        "\n",
        "        # Extract title\n",
        "        title_match = re.search(r'DESCRIPTIVE TITLE OF APPLICANT\\'S PROJECT\\*\\s*([^\\n]+)', text)\n",
        "        if not title_match:\n",
        "            title_match = re.search(r'Title:\\s*([^\\n]+)', text)\n",
        "        basic_info['Project_Title'] = title_match.group(1).strip() if title_match else ''\n",
        "\n",
        "        # Extract organization\n",
        "        org_match = re.search(r'Legal Name\\*:\\s*([^\\n]+)', text)\n",
        "        basic_info['Organization'] = org_match.group(1).strip() if org_match else ''\n",
        "\n",
        "        # Extract project dates\n",
        "        start_date_match = re.search(r'Start Date\\*\\s*([0-9/]+)', text)\n",
        "        end_date_match = re.search(r'Ending Date\\*\\s*([0-9/]+)', text)\n",
        "        basic_info['Start_Date'] = start_date_match.group(1) if start_date_match else ''\n",
        "        basic_info['End_Date'] = end_date_match.group(1) if end_date_match else ''\n",
        "\n",
        "        return basic_info\n",
        "\n",
        "    def extract_personnel(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extract key personnel information\"\"\"\n",
        "        personnel = []\n",
        "\n",
        "        # Find personnel section\n",
        "        personnel_patterns = [\n",
        "            r'Senior/Key Personnel:\\s*Organization:\\s*Role Category:(.*?)(?=\\n\\n|\\nAlways follow)',\n",
        "            r'PROFILE - Project Director/Principal Investigator(.*?)(?=PROFILE - Senior/Key Person|$)',\n",
        "            r'PROFILE - Senior/Key Person(.*?)(?=PROFILE - Senior/Key Person|$)'\n",
        "        ]\n",
        "\n",
        "        for pattern in personnel_patterns:\n",
        "            matches = re.finditer(pattern, text, re.DOTALL)\n",
        "            for match in matches:\n",
        "                person_text = match.group(1)\n",
        "                person_info = self._parse_person_info(person_text)\n",
        "                if person_info:\n",
        "                    personnel.append(person_info)\n",
        "\n",
        "        return personnel\n",
        "\n",
        "    def _parse_person_info(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Parse individual person information\"\"\"\n",
        "        person = {}\n",
        "\n",
        "        # Extract name\n",
        "        name_match = re.search(r'First Name\\*:\\s*([^\\n]+).*?Last Name\\*:\\s*([^\\n]+)', text, re.DOTALL)\n",
        "        if name_match:\n",
        "            person['First_Name'] = name_match.group(1).strip()\n",
        "            person['Last_Name'] = name_match.group(2).strip()\n",
        "            person['Full_Name'] = f\"{person['First_Name']} {person['Last_Name']}\"\n",
        "\n",
        "        # Extract role\n",
        "        role_match = re.search(r'Project Role\\*:\\s*([^\\n]+)', text)\n",
        "        person['Role'] = role_match.group(1).strip() if role_match else ''\n",
        "\n",
        "        # Extract organization\n",
        "        org_match = re.search(r'Organization Name\\*:\\s*([^\\n]+)', text)\n",
        "        person['Organization'] = org_match.group(1).strip() if org_match else ''\n",
        "\n",
        "        # Extract position\n",
        "        position_match = re.search(r'Position/Title\\*:\\s*([^\\n]+)', text)\n",
        "        person['Position'] = position_match.group(1).strip() if position_match else ''\n",
        "\n",
        "        # Extract degree\n",
        "        degree_match = re.search(r'Degree Type:\\s*([^\\n]+)', text)\n",
        "        person['Degree'] = degree_match.group(1).strip() if degree_match else ''\n",
        "\n",
        "        return person if person else None\n",
        "\n",
        "    def extract_budget_info(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract budget information\"\"\"\n",
        "        budget_info = {}\n",
        "\n",
        "        # Extract total federal funds\n",
        "        federal_funds_match = re.search(r'Total Federal Funds Requested\\*\\s*\\$([0-9,]+)', text)\n",
        "        budget_info['Total_Federal_Funds'] = federal_funds_match.group(1) if federal_funds_match else ''\n",
        "\n",
        "        # Extract total project funding\n",
        "        total_funds_match = re.search(r'Total Federal & Non-Federal Funds\\*\\s*\\$([0-9,]+)', text)\n",
        "        budget_info['Total_Project_Funds'] = total_funds_match.group(1) if total_funds_match else ''\n",
        "\n",
        "        # Extract budget periods (simplified)\n",
        "        budget_periods = re.findall(r'Budget Period ([0-9]+).*?\\$([0-9,]+)', text)\n",
        "        for period, amount in budget_periods:\n",
        "            budget_info[f'Budget_Period_{period}'] = amount\n",
        "\n",
        "        return budget_info\n",
        "\n",
        "    def extract_research_info(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract research-specific information\"\"\"\n",
        "        research_info = {}\n",
        "\n",
        "        # Extract abstract/summary\n",
        "        abstract_match = re.search(r'Summary:(.*?)(?=Project Narrative|Narrative:|$)', text, re.DOTALL)\n",
        "        if abstract_match:\n",
        "            research_info['Abstract'] = abstract_match.group(1).strip()[:1000]  # Limit length\n",
        "\n",
        "        # Extract specific aims\n",
        "        aims_match = re.search(r'Specific Aims?\\.(.*?)(?=Research Strategy|B\\. Innovation|$)', text, re.DOTALL)\n",
        "        if aims_match:\n",
        "            research_info['Specific_Aims'] = aims_match.group(1).strip()[:1000]\n",
        "\n",
        "        # Extract human subjects info\n",
        "        human_subjects_match = re.search(r'Are Human Subjects Involved\\?\\*\\s*(Yes|No)', text)\n",
        "        research_info['Human_Subjects'] = human_subjects_match.group(1) if human_subjects_match else ''\n",
        "\n",
        "        # Extract vertebrate animals info\n",
        "        animals_match = re.search(r'Are Vertebrate Animals Used\\?\\*\\s*(Yes|No)', text)\n",
        "        research_info['Vertebrate_Animals'] = animals_match.group(1) if animals_match else ''\n",
        "\n",
        "        # Extract clinical trial info\n",
        "        clinical_trial_match = re.search(r'Clinical Trial:\\s*(Y|N)', text)\n",
        "        research_info['Clinical_Trial'] = clinical_trial_match.group(1) if clinical_trial_match else ''\n",
        "\n",
        "        return research_info\n",
        "\n",
        "    def extract_sbir_info(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract SBIR/STTR specific information\"\"\"\n",
        "        sbir_info = {}\n",
        "\n",
        "        # Extract program type\n",
        "        program_match = re.search(r'Program Type.*?(SBIR|STTR)', text)\n",
        "        sbir_info['Program_Type'] = program_match.group(1) if program_match else ''\n",
        "\n",
        "        # Extract application type\n",
        "        app_type_match = re.search(r'Application Type.*?(Phase I|Phase II|Fast-Track)', text)\n",
        "        sbir_info['Application_Type'] = app_type_match.group(1) if app_type_match else ''\n",
        "\n",
        "        # Extract small business info\n",
        "        sb_match = re.search(r'small business eligibility.*?(Yes|No)', text)\n",
        "        sbir_info['Small_Business_Eligible'] = sb_match.group(1) if sb_match else ''\n",
        "\n",
        "        return sbir_info\n",
        "\n",
        "    def parse_pdf_text(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF\"\"\"\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF: {e}\")\n",
        "        return text\n",
        "\n",
        "    def parse_grant_application(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Main parsing function\"\"\"\n",
        "        print(\"Extracting text from PDF...\")\n",
        "        text = self.parse_pdf_text(pdf_path)\n",
        "\n",
        "        print(\"Parsing basic information...\")\n",
        "        basic_info = self.extract_basic_info(text)\n",
        "\n",
        "        print(\"Parsing personnel information...\")\n",
        "        personnel = self.extract_personnel(text)\n",
        "\n",
        "        print(\"Parsing budget information...\")\n",
        "        budget_info = self.extract_budget_info(text)\n",
        "\n",
        "        print(\"Parsing research information...\")\n",
        "        research_info = self.extract_research_info(text)\n",
        "\n",
        "        print(\"Parsing SBIR information...\")\n",
        "        sbir_info = self.extract_sbir_info(text)\n",
        "\n",
        "        # Combine all information\n",
        "        grant_data = {\n",
        "            **basic_info,\n",
        "            **budget_info,\n",
        "            **research_info,\n",
        "            **sbir_info\n",
        "        }\n",
        "\n",
        "        return grant_data, personnel\n",
        "\n",
        "    def save_to_csv(self, pdf_path: str, output_prefix: str = \"nih_grant\"):\n",
        "        \"\"\"Parse PDF and save to CSV files\"\"\"\n",
        "        grant_data, personnel = self.parse_grant_application(pdf_path)\n",
        "\n",
        "        # Save main grant information\n",
        "        grant_df = pd.DataFrame([grant_data])\n",
        "        grant_df.to_csv(f\"{output_prefix}_main.csv\", index=False)\n",
        "        print(f\"Main grant data saved to {output_prefix}_main.csv\")\n",
        "\n",
        "        # Save personnel information\n",
        "        if personnel:\n",
        "            personnel_df = pd.DataFrame(personnel)\n",
        "            personnel_df.to_csv(f\"{output_prefix}_personnel.csv\", index=False)\n",
        "            print(f\"Personnel data saved to {output_prefix}_personnel.csv\")\n",
        "\n",
        "        # Save summary information\n",
        "        summary_data = {\n",
        "            'Field': list(grant_data.keys()),\n",
        "            'Value': list(grant_data.values())\n",
        "        }\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_df.to_csv(f\"{output_prefix}_summary.csv\", index=False)\n",
        "        print(f\"Summary data saved to {output_prefix}_summary.csv\")\n",
        "\n",
        "        return grant_df, personnel_df if personnel else None\n",
        "\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    parser = NIHGrantParser()\n",
        "\n",
        "    # Replace with your PDF path\n",
        "    pdf_path = \"harty-application.pdf\"\n",
        "\n",
        "    try:\n",
        "        grant_df, personnel_df = parser.save_to_csv(pdf_path, \"nih_grant_harty\")\n",
        "\n",
        "        print(\"\\nExtracted Grant Information:\")\n",
        "        print(grant_df.head())\n",
        "\n",
        "        if personnel_df is not None:\n",
        "            print(\"\\nExtracted Personnel Information:\")\n",
        "            print(personnel_df.head())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import re\n",
        "from typing import Dict, List, Any, Optional, Callable\n",
        "import json\n",
        "import yaml\n",
        "from dataclasses import dataclass, field\n",
        "from abc import ABC, abstractmethod\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ExtractionRule:\n",
        "    \"\"\"Defines a rule for extracting data from text\"\"\"\n",
        "    name: str\n",
        "    pattern: str\n",
        "    extraction_type: str = \"single\"  # single, multiple, table\n",
        "    post_process: Optional[str] = None\n",
        "    required: bool = False\n",
        "    default_value: Any = \"\"\n",
        "    max_length: Optional[int] = None\n",
        "\n",
        "@dataclass\n",
        "class TableExtractionRule:\n",
        "    \"\"\"Defines rules for extracting tabular data\"\"\"\n",
        "    name: str\n",
        "    start_pattern: str\n",
        "    end_pattern: str\n",
        "    column_patterns: List[str]\n",
        "    row_separator: str = \"\\n\"\n",
        "\n",
        "@dataclass\n",
        "class SectionConfig:\n",
        "    \"\"\"Configuration for a document section\"\"\"\n",
        "    name: str\n",
        "    start_pattern: Optional[str] = None\n",
        "    end_pattern: Optional[str] = None\n",
        "    extraction_rules: List[ExtractionRule] = field(default_factory=list)\n",
        "    table_rules: List[TableExtractionRule] = field(default_factory=list)\n",
        "    subsections: List['SectionConfig'] = field(default_factory=list)\n",
        "\n",
        "class TextExtractor(ABC):\n",
        "    \"\"\"Abstract base class for text extraction\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def extract_text(self, file_path: str) -> str:\n",
        "        pass\n",
        "\n",
        "class PDFExtractor(TextExtractor):\n",
        "    \"\"\"PDF text extraction using PyPDF2\"\"\"\n",
        "\n",
        "    def extract_text(self, file_path: str) -> str:\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                for page_num, page in enumerate(pdf_reader.pages):\n",
        "                    try:\n",
        "                        page_text = page.extract_text()\n",
        "                        text += f\"\\n--- PAGE {page_num + 1} ---\\n{page_text}\\n\"\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Could not extract text from page {page_num + 1}: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading PDF: {e}\")\n",
        "            raise\n",
        "        return text\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Handles post-processing of extracted data\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text: str, max_length: Optional[int] = None) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # Limit length if specified\n",
        "        if max_length and len(text) > max_length:\n",
        "            text = text[:max_length] + \"...\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_numbers(text: str) -> str:\n",
        "        \"\"\"Extract numeric values from text\"\"\"\n",
        "        numbers = re.findall(r'[\\d,]+\\.?\\d*', text)\n",
        "        return numbers[0] if numbers else \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_dates(text: str) -> str:\n",
        "        \"\"\"Extract date patterns from text\"\"\"\n",
        "        date_patterns = [\n",
        "            r'\\d{1,2}/\\d{1,2}/\\d{4}',\n",
        "            r'\\d{4}-\\d{2}-\\d{2}',\n",
        "            r'\\w+ \\d{1,2}, \\d{4}'\n",
        "        ]\n",
        "        for pattern in date_patterns:\n",
        "            dates = re.findall(pattern, text)\n",
        "            if dates:\n",
        "                return dates[0]\n",
        "        return \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_emails(text: str) -> List[str]:\n",
        "        \"\"\"Extract email addresses from text\"\"\"\n",
        "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "        return re.findall(email_pattern, text)\n",
        "\n",
        "class DataExtractor:\n",
        "    \"\"\"Main data extraction engine\"\"\"\n",
        "\n",
        "    def __init__(self, processor: DataProcessor = None):\n",
        "        self.processor = processor or DataProcessor()\n",
        "\n",
        "    def extract_by_rule(self, text: str, rule: ExtractionRule) -> Any:\n",
        "        \"\"\"Extract data based on a single rule\"\"\"\n",
        "        try:\n",
        "            if rule.extraction_type == \"single\":\n",
        "                match = re.search(rule.pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "                if match:\n",
        "                    result = match.group(1) if match.groups() else match.group(0)\n",
        "                else:\n",
        "                    result = rule.default_value\n",
        "\n",
        "            elif rule.extraction_type == \"multiple\":\n",
        "                matches = re.findall(rule.pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "                result = matches if matches else [rule.default_value]\n",
        "\n",
        "            elif rule.extraction_type == \"all_text\":\n",
        "                result = text\n",
        "\n",
        "            else:\n",
        "                result = rule.default_value\n",
        "\n",
        "            # Post-processing\n",
        "            if rule.post_process and isinstance(result, str):\n",
        "                if rule.post_process == \"clean_text\":\n",
        "                    result = self.processor.clean_text(result, rule.max_length)\n",
        "                elif rule.post_process == \"extract_numbers\":\n",
        "                    result = self.processor.extract_numbers(result)\n",
        "                elif rule.post_process == \"extract_dates\":\n",
        "                    result = self.processor.extract_dates(result)\n",
        "                elif rule.post_process == \"extract_emails\":\n",
        "                    result = self.processor.extract_emails(result)\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error extracting data for rule {rule.name}: {e}\")\n",
        "            return rule.default_value\n",
        "\n",
        "    def extract_table_data(self, text: str, table_rule: TableExtractionRule) -> List[Dict[str, str]]:\n",
        "        \"\"\"Extract tabular data\"\"\"\n",
        "        try:\n",
        "            # Find table boundaries\n",
        "            start_match = re.search(table_rule.start_pattern, text, re.IGNORECASE)\n",
        "            end_match = re.search(table_rule.end_pattern, text, re.IGNORECASE)\n",
        "\n",
        "            if not start_match:\n",
        "                return []\n",
        "\n",
        "            table_start = start_match.end()\n",
        "            table_end = end_match.start() if end_match else len(text)\n",
        "            table_text = text[table_start:table_end]\n",
        "\n",
        "            # Split into rows\n",
        "            rows = table_text.split(table_rule.row_separator)\n",
        "\n",
        "            # Extract data using column patterns\n",
        "            table_data = []\n",
        "            for row in rows:\n",
        "                row_data = {}\n",
        "                for i, pattern in enumerate(table_rule.column_patterns):\n",
        "                    match = re.search(pattern, row, re.IGNORECASE)\n",
        "                    column_name = f\"column_{i}\" if i < len(table_rule.column_patterns) else f\"column_{i}\"\n",
        "                    row_data[column_name] = match.group(1) if match and match.groups() else \"\"\n",
        "\n",
        "                # Only add non-empty rows\n",
        "                if any(row_data.values()):\n",
        "                    table_data.append(row_data)\n",
        "\n",
        "            return table_data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error extracting table data for {table_rule.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "class DocumentPipeline:\n",
        "    \"\"\"Main pipeline for processing documents\"\"\"\n",
        "\n",
        "    def __init__(self, config_path: Optional[str] = None, text_extractor: TextExtractor = None):\n",
        "        self.text_extractor = text_extractor or PDFExtractor()\n",
        "        self.data_extractor = DataExtractor()\n",
        "        self.config = self.load_config(config_path) if config_path else None\n",
        "\n",
        "    def load_config(self, config_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Load configuration from file\"\"\"\n",
        "        config_path = Path(config_path)\n",
        "\n",
        "        if config_path.suffix.lower() == '.json':\n",
        "            with open(config_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        elif config_path.suffix.lower() in ['.yml', '.yaml']:\n",
        "            with open(config_path, 'r') as f:\n",
        "                return yaml.safe_load(f)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported config file format: {config_path.suffix}\")\n",
        "\n",
        "    def create_extraction_rules_from_config(self, config: Dict[str, Any]) -> List[SectionConfig]:\n",
        "        \"\"\"Convert configuration to extraction rules\"\"\"\n",
        "        sections = []\n",
        "\n",
        "        for section_data in config.get('sections', []):\n",
        "            # Create extraction rules\n",
        "            extraction_rules = []\n",
        "            for rule_data in section_data.get('extraction_rules', []):\n",
        "                rule = ExtractionRule(**rule_data)\n",
        "                extraction_rules.append(rule)\n",
        "\n",
        "            # Create table rules\n",
        "            table_rules = []\n",
        "            for table_data in section_data.get('table_rules', []):\n",
        "                table_rule = TableExtractionRule(**table_data)\n",
        "                table_rules.append(table_rule)\n",
        "\n",
        "            # Create section config\n",
        "            section = SectionConfig(\n",
        "                name=section_data['name'],\n",
        "                start_pattern=section_data.get('start_pattern'),\n",
        "                end_pattern=section_data.get('end_pattern'),\n",
        "                extraction_rules=extraction_rules,\n",
        "                table_rules=table_rules\n",
        "            )\n",
        "            sections.append(section)\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def extract_section_text(self, full_text: str, section: SectionConfig) -> str:\n",
        "        \"\"\"Extract text for a specific section\"\"\"\n",
        "        if not section.start_pattern:\n",
        "            return full_text\n",
        "\n",
        "        start_match = re.search(section.start_pattern, full_text, re.DOTALL | re.IGNORECASE)\n",
        "        if not start_match:\n",
        "            logger.warning(f\"Could not find start pattern for section: {section.name}\")\n",
        "            return \"\"\n",
        "\n",
        "        section_start = start_match.start()\n",
        "\n",
        "        if section.end_pattern:\n",
        "            end_match = re.search(section.end_pattern, full_text[section_start:], re.DOTALL | re.IGNORECASE)\n",
        "            section_end = section_start + end_match.start() if end_match else len(full_text)\n",
        "        else:\n",
        "            section_end = len(full_text)\n",
        "\n",
        "        return full_text[section_start:section_end]\n",
        "\n",
        "    def process_document(self, file_path: str, sections: List[SectionConfig] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Process a document and extract structured data\"\"\"\n",
        "        logger.info(f\"Processing document: {file_path}\")\n",
        "\n",
        "        # Extract text\n",
        "        full_text = self.text_extractor.extract_text(file_path)\n",
        "\n",
        "        if not sections and self.config:\n",
        "            sections = self.create_extraction_rules_from_config(self.config)\n",
        "        elif not sections:\n",
        "            raise ValueError(\"No extraction configuration provided\")\n",
        "\n",
        "        extracted_data = {}\n",
        "        all_tables = {}\n",
        "\n",
        "        for section in sections:\n",
        "            logger.info(f\"Processing section: {section.name}\")\n",
        "\n",
        "            # Get section text\n",
        "            section_text = self.extract_section_text(full_text, section)\n",
        "\n",
        "            # Extract data using rules\n",
        "            section_data = {}\n",
        "            for rule in section.extraction_rules:\n",
        "                section_data[rule.name] = self.data_extractor.extract_by_rule(section_text, rule)\n",
        "\n",
        "            # Extract table data\n",
        "            for table_rule in section.table_rules:\n",
        "                table_data = self.data_extractor.extract_table_data(section_text, table_rule)\n",
        "                all_tables[f\"{section.name}_{table_rule.name}\"] = table_data\n",
        "\n",
        "            extracted_data[section.name] = section_data\n",
        "\n",
        "        return {\n",
        "            'sections': extracted_data,\n",
        "            'tables': all_tables,\n",
        "            'metadata': {\n",
        "                'file_path': file_path,\n",
        "                'extraction_timestamp': pd.Timestamp.now().isoformat()\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def save_to_csv(self, extracted_data: Dict[str, Any], output_prefix: str):\n",
        "        \"\"\"Save extracted data to CSV files\"\"\"\n",
        "        output_prefix = Path(output_prefix)\n",
        "\n",
        "        # Save main sections data\n",
        "        sections_data = []\n",
        "        for section_name, section_data in extracted_data['sections'].items():\n",
        "            flat_data = {'section': section_name}\n",
        "            flat_data.update(section_data)\n",
        "            sections_data.append(flat_data)\n",
        "\n",
        "        if sections_data:\n",
        "            sections_df = pd.DataFrame(sections_data)\n",
        "            sections_file = f\"{output_prefix}_sections.csv\"\n",
        "            sections_df.to_csv(sections_file, index=False)\n",
        "            logger.info(f\"Sections data saved to {sections_file}\")\n",
        "\n",
        "        # Save tables data\n",
        "        for table_name, table_data in extracted_data['tables'].items():\n",
        "            if table_data:\n",
        "                table_df = pd.DataFrame(table_data)\n",
        "                table_file = f\"{output_prefix}_table_{table_name}.csv\"\n",
        "                table_df.to_csv(table_file, index=False)\n",
        "                logger.info(f\"Table data saved to {table_file}\")\n",
        "\n",
        "        # Save flattened data\n",
        "        flattened_data = {}\n",
        "        for section_name, section_data in extracted_data['sections'].items():\n",
        "            for key, value in section_data.items():\n",
        "                flattened_key = f\"{section_name}_{key}\"\n",
        "                if isinstance(value, list):\n",
        "                    flattened_data[flattened_key] = \"; \".join(map(str, value))\n",
        "                else:\n",
        "                    flattened_data[flattened_key] = value\n",
        "\n",
        "        # Add metadata\n",
        "        flattened_data.update(extracted_data['metadata'])\n",
        "\n",
        "        flattened_df = pd.DataFrame([flattened_data])\n",
        "        flattened_file = f\"{output_prefix}_flattened.csv\"\n",
        "        flattened_df.to_csv(flattened_file, index=False)\n",
        "        logger.info(f\"Flattened data saved to {flattened_file}\")\n",
        "\n",
        "        return flattened_df\n",
        "\n",
        "# Configuration builder helper\n",
        "class ConfigBuilder:\n",
        "    \"\"\"Helper class to build extraction configurations\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_nih_grant_config() -> Dict[str, Any]:\n",
        "        \"\"\"Create configuration for NIH grant applications\"\"\"\n",
        "        return {\n",
        "            \"document_type\": \"nih_grant\",\n",
        "            \"sections\": [\n",
        "                {\n",
        "                    \"name\": \"basic_info\",\n",
        "                    \"start_pattern\": r\"APPLICATION FOR FEDERAL ASSISTANCE\",\n",
        "                    \"end_pattern\": r\"Table of Contents\",\n",
        "                    \"extraction_rules\": [\n",
        "                        {\n",
        "                            \"name\": \"pi_name\",\n",
        "                            \"pattern\": r\"Contact PD/PI:\\s*([^,\\n]+)\",\n",
        "                            \"post_process\": \"clean_text\",\n",
        "                            \"required\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"application_id\",\n",
        "                            \"pattern\": r\"Application Identifier\\s*([A-Z0-9]+)\",\n",
        "                            \"required\": True\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"project_title\",\n",
        "                            \"pattern\": r\"DESCRIPTIVE TITLE.*?\\n([^\\n]+)\",\n",
        "                            \"post_process\": \"clean_text\",\n",
        "                            \"max_length\": 200\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"organization\",\n",
        "                            \"pattern\": r\"Legal Name\\*:\\s*([^\\n]+)\",\n",
        "                            \"post_process\": \"clean_text\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"start_date\",\n",
        "                            \"pattern\": r\"Start Date\\*\\s*([0-9/]+)\",\n",
        "                            \"post_process\": \"extract_dates\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"end_date\",\n",
        "                            \"pattern\": r\"Ending Date\\*\\s*([0-9/]+)\",\n",
        "                            \"post_process\": \"extract_dates\"\n",
        "                        }\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"name\": \"personnel\",\n",
        "                    \"start_pattern\": r\"Senior/Key Personnel:\",\n",
        "                    \"end_pattern\": r\"Research & Related Budget\",\n",
        "                    \"extraction_rules\": [\n",
        "                        {\n",
        "                            \"name\": \"personnel_names\",\n",
        "                            \"pattern\": r\"([A-Z][a-z]+ [A-Z][a-z]+) Ph\\.D\",\n",
        "                            \"extraction_type\": \"multiple\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"organizations\",\n",
        "                            \"pattern\": r\"([A-Z][a-z]+ [A-Za-z ]+(?:University|Institute|Center))\",\n",
        "                            \"extraction_type\": \"multiple\"\n",
        "                        }\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"name\": \"budget\",\n",
        "                    \"start_pattern\": r\"Total Federal Funds Requested\",\n",
        "                    \"end_pattern\": r\"SBIR/STTR Information\",\n",
        "                    \"extraction_rules\": [\n",
        "                        {\n",
        "                            \"name\": \"total_federal_funds\",\n",
        "                            \"pattern\": r\"Total Federal Funds Requested\\*\\s*\\$([0-9,]+)\",\n",
        "                            \"post_process\": \"extract_numbers\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"total_project_funds\",\n",
        "                            \"pattern\": r\"Total Federal & Non-Federal Funds\\*\\s*\\$([0-9,]+)\",\n",
        "                            \"post_process\": \"extract_numbers\"\n",
        "                        }\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"name\": \"research_info\",\n",
        "                    \"start_pattern\": r\"Project Summary/Abstract\",\n",
        "                    \"end_pattern\": r\"Bibliography\",\n",
        "                    \"extraction_rules\": [\n",
        "                        {\n",
        "                            \"name\": \"abstract\",\n",
        "                            \"pattern\": r\"Summary:(.*?)(?=Project Narrative|Narrative:)\",\n",
        "                            \"post_process\": \"clean_text\",\n",
        "                            \"max_length\": 1000\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"human_subjects\",\n",
        "                            \"pattern\": r\"Are Human Subjects Involved\\?\\*\\s*(Yes|No)\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"name\": \"vertebrate_animals\",\n",
        "                            \"pattern\": r\"Are Vertebrate Animals Used\\?\\*\\s*(Yes|No)\"\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def save_config(config: Dict[str, Any], file_path: str):\n",
        "        \"\"\"Save configuration to file\"\"\"\n",
        "        file_path = Path(file_path)\n",
        "\n",
        "        if file_path.suffix.lower() == '.json':\n",
        "            with open(file_path, 'w') as f:\n",
        "                json.dump(config, f, indent=2)\n",
        "        elif file_path.suffix.lower() in ['.yml', '.yaml']:\n",
        "            with open(file_path, 'w') as f:\n",
        "                yaml.dump(config, f, default_flow_style=False)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported config file format: {file_path.suffix}\")\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Create and save NIH grant configuration\n",
        "    config = ConfigBuilder.create_nih_grant_config()\n",
        "    ConfigBuilder.save_config(config, \"nih_grant_config.json\")\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipeline = DocumentPipeline(config_path=\"nih_grant_config.json\")\n",
        "\n",
        "    # Process document\n",
        "    try:\n",
        "        pdf_path = \"harty-application.pdf\"  # Replace with your PDF path\n",
        "        extracted_data = pipeline.process_document(pdf_path)\n",
        "\n",
        "        # Save to CSV\n",
        "        output_files = pipeline.save_to_csv(extracted_data, \"extracted_grant_data\")\n",
        "\n",
        "        print(\"Processing completed successfully!\")\n",
        "        print(f\"Extracted data from {len(extracted_data['sections'])} sections\")\n",
        "        print(f\"Found {len(extracted_data['tables'])} tables\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing document: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGJuXTMKo4R3",
        "outputId": "63adea2e-bf34-4861-a11b-2dca3e0323f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Could not find start pattern for section: personnel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed successfully!\n",
            "Extracted data from 4 sections\n",
            "Found 0 tables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import PyPDF2\n",
        "from typing import Dict, Any, List, Optional\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ExtractionConfig:\n",
        "    \"\"\"Configuration for data extraction\"\"\"\n",
        "    field_name: str\n",
        "    patterns: List[str]\n",
        "    extraction_type: str = \"single\"  # single, multiple, nested, table\n",
        "    post_process: Optional[str] = None\n",
        "    required: bool = False\n",
        "    default_value: Any = \"\"\n",
        "\n",
        "class JSONDocumentExtractor:\n",
        "    \"\"\"Generalizable document extractor that outputs structured JSON\"\"\"\n",
        "\n",
        "    def __init__(self, document_type: str = \"nsf_grant\"):\n",
        "        self.document_type = document_type\n",
        "        self.extraction_configs = self._load_extraction_configs()\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF file\"\"\"\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                for page_num, page in enumerate(pdf_reader.pages):\n",
        "                    try:\n",
        "                        page_text = page.extract_text()\n",
        "                        text += f\"\\n{page_text}\\n\"\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Could not extract text from page {page_num + 1}: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading PDF: {e}\")\n",
        "            raise\n",
        "        return text\n",
        "\n",
        "    def _load_extraction_configs(self) -> Dict[str, List[ExtractionConfig]]:\n",
        "        \"\"\"Load extraction configurations based on document type\"\"\"\n",
        "        if self.document_type == \"nsf_grant\":\n",
        "            return self._get_nsf_grant_configs()\n",
        "        elif self.document_type == \"nih_grant\":\n",
        "            return self._get_nih_grant_configs()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported document type: {self.document_type}\")\n",
        "\n",
        "    def _get_nsf_grant_configs(self) -> Dict[str, List[ExtractionConfig]]:\n",
        "        \"\"\"NSF Grant proposal extraction configurations\"\"\"\n",
        "        return {\n",
        "            \"administrative_info\": [\n",
        "                ExtractionConfig(\"proposal_id\", [r\"Proposal No\\.?\\s*:?\\s*([A-Z0-9-]+)\", r\"NSF Proposal.*?([0-9]{7})\"]),\n",
        "                ExtractionConfig(\"nsf_program\", [r\"Program:\\s*([^\\n]+)\", r\"NSF Program.*?:\\s*([^\\n]+)\"]),\n",
        "                ExtractionConfig(\"submission_date\", [r\"Submitted.*?:\\s*([0-9/\\-]+)\", r\"Date Submitted.*?:\\s*([0-9/\\-]+)\"]),\n",
        "                ExtractionConfig(\"proposal_title\", [r\"Project Title:\\s*([^\\n]+)\", r\"Title:\\s*([^\\n]+)\"]),\n",
        "                ExtractionConfig(\"pi_name\", [r\"Principal Investigator.*?:\\s*([^\\n,]+)\", r\"PI:\\s*([^\\n,]+)\"]),\n",
        "                ExtractionConfig(\"pi_email\", [r\"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})\", r\"Email.*?:\\s*([^\\s\\n]+@[^\\s\\n]+)\"]),\n",
        "                ExtractionConfig(\"institution\", [r\"Institution.*?:\\s*([^\\n]+)\", r\"Organization.*?:\\s*([^\\n]+)\"]),\n",
        "                ExtractionConfig(\"requested_amount\", [r\"Total Budget.*?:?\\s*\\$?([0-9,]+)\", r\"Requested Amount.*?:?\\s*\\$?([0-9,]+)\"]),\n",
        "            ],\n",
        "            \"budget\": [\n",
        "                ExtractionConfig(\"total_budget\", [r\"Total.*?Budget.*?:?\\s*\\$?([0-9,]+)\"]),\n",
        "                ExtractionConfig(\"personnel_costs\", [r\"Personnel.*?:?\\s*\\$?([0-9,]+)\"]),\n",
        "                ExtractionConfig(\"equipment_costs\", [r\"Equipment.*?:?\\s*\\$?([0-9,]+)\"]),\n",
        "                ExtractionConfig(\"travel_costs\", [r\"Travel.*?:?\\s*\\$?([0-9,]+)\"]),\n",
        "                ExtractionConfig(\"indirect_costs\", [r\"Indirect.*?Cost.*?:?\\s*\\$?([0-9,]+)\", r\"F&A.*?:?\\s*\\$?([0-9,]+)\"]),\n",
        "            ],\n",
        "            \"project_description\": [\n",
        "                ExtractionConfig(\"overview\", [r\"Project Summary.*?:\\s*(.*?)(?=Intellectual Merit|Project Description)\", r\"Overview.*?:\\s*(.*?)(?=\\n\\n|\\n[A-Z])\"]),\n",
        "                ExtractionConfig(\"intellectual_merit\", [r\"Intellectual Merit.*?:\\s*(.*?)(?=Broader Impact|References)\", r\"Merit.*?:\\s*(.*?)(?=\\n\\n|\\n[A-Z])\"]),\n",
        "                ExtractionConfig(\"broader_impacts\", [r\"Broader Impact.*?:\\s*(.*?)(?=References|Bibliography)\", r\"Impact.*?:\\s*(.*?)(?=\\n\\n|\\n[A-Z])\"]),\n",
        "                ExtractionConfig(\"keywords\", [r\"Keywords?.*?:\\s*([^\\n]+)\", r\"Key words?.*?:\\s*([^\\n]+)\"], extraction_type=\"multiple\"),\n",
        "            ],\n",
        "            \"research_plan\": [\n",
        "                ExtractionConfig(\"objectives\", [r\"Objective.*?:\\s*(.*?)(?=Methodology|Approach)\", r\"Goals?.*?:\\s*(.*?)(?=\\n\\n|\\n[A-Z])\"], extraction_type=\"multiple\"),\n",
        "                ExtractionConfig(\"methodology\", [r\"Methodology.*?:\\s*(.*?)(?=Timeline|Expected)\", r\"Approach.*?:\\s*(.*?)(?=\\n\\n|\\n[A-Z])\"]),\n",
        "                ExtractionConfig(\"timeline\", [r\"Timeline.*?:\\s*(.*?)(?=Expected|Results)\", r\"Schedule.*?:\\s*(.*?)(?=\\n\\n|\\n[A-Z])\"]),\n",
        "                ExtractionConfig(\"innovation\", [r\"Innovation.*?:\\s*(.*?)(?=Significance|Impact)\", r\"Novel.*?:\\s*(.*?)(?=\\n\\n|\\n[A-Z])\"]),\n",
        "            ],\n",
        "            \"compliance\": [\n",
        "                ExtractionConfig(\"human_subjects\", [r\"Human Subjects.*?:\\s*(Yes|No)\", r\"IRB.*?:\\s*(Yes|No|Approved|Pending)\"]),\n",
        "                ExtractionConfig(\"vertebrate_animals\", [r\"Vertebrate Animals.*?:\\s*(Yes|No)\", r\"IACUC.*?:\\s*(Yes|No|Approved|Pending)\"]),\n",
        "                ExtractionConfig(\"biohazards\", [r\"Biohazard.*?:\\s*(Yes|No)\", r\"Biological.*?Agent.*?:\\s*(Yes|No)\"]),\n",
        "            ],\n",
        "            \"personnel\": [\n",
        "                ExtractionConfig(\"co_investigators\", [r\"Co-?PI.*?:\\s*([^\\n]+)\", r\"Co-?Investigator.*?:\\s*([^\\n]+)\"], extraction_type=\"multiple\"),\n",
        "                ExtractionConfig(\"senior_personnel\", [r\"Senior Personnel.*?:\\s*([^\\n]+)\"], extraction_type=\"multiple\"),\n",
        "                ExtractionConfig(\"collaborators\", [r\"Collaborator.*?:\\s*([^\\n]+)\", r\"Partner.*?:\\s*([^\\n]+)\"], extraction_type=\"multiple\"),\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def _get_nih_grant_configs(self) -> Dict[str, List[ExtractionConfig]]:\n",
        "        \"\"\"NIH Grant application extraction configurations\"\"\"\n",
        "        return {\n",
        "            \"administrative_info\": [\n",
        "                ExtractionConfig(\"application_id\", [r\"Application.*?ID.*?:\\s*([A-Z0-9-]+)\", r\"Grant.*?Number.*?:\\s*([A-Z0-9-]+)\"]),\n",
        "                ExtractionConfig(\"funding_opportunity\", [r\"Funding Opportunity.*?:\\s*([A-Z0-9-]+)\", r\"FOA.*?:\\s*([A-Z0-9-]+)\"]),\n",
        "                ExtractionConfig(\"pi_name\", [r\"Principal Investigator.*?:\\s*([^\\n,]+)\", r\"PI.*?:\\s*([^\\n,]+)\"]),\n",
        "                ExtractionConfig(\"institution\", [r\"Institution.*?:\\s*([^\\n]+)\", r\"Organization.*?:\\s*([^\\n]+)\"]),\n",
        "                ExtractionConfig(\"project_title\", [r\"Project Title.*?:\\s*([^\\n]+)\", r\"Title.*?:\\s*([^\\n]+)\"]),\n",
        "            ],\n",
        "            \"research_info\": [\n",
        "                ExtractionConfig(\"specific_aims\", [r\"Specific Aims.*?:\\s*(.*?)(?=Research Strategy|Background)\"]),\n",
        "                ExtractionConfig(\"significance\", [r\"Significance.*?:\\s*(.*?)(?=Innovation|Approach)\"]),\n",
        "                ExtractionConfig(\"innovation\", [r\"Innovation.*?:\\s*(.*?)(?=Approach|Methods)\"]),\n",
        "                ExtractionConfig(\"approach\", [r\"Approach.*?:\\s*(.*?)(?=References|Bibliography)\"]),\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def extract_single_value(self, text: str, patterns: List[str]) -> str:\n",
        "        \"\"\"Extract single value using multiple pattern attempts\"\"\"\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                result = match.group(1) if match.groups() else match.group(0)\n",
        "                return self._clean_text(result)\n",
        "        return \"\"\n",
        "\n",
        "    def extract_multiple_values(self, text: str, patterns: List[str]) -> List[str]:\n",
        "        \"\"\"Extract multiple values using patterns\"\"\"\n",
        "        results = []\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "            results.extend([self._clean_text(match) for match in matches])\n",
        "        return list(set(results))  # Remove duplicates\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize extracted text\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove extra whitespace and newlines\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # Remove common artifacts\n",
        "        text = re.sub(r'[^\\w\\s\\-.,;:()$@]', '', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_section_data(self, text: str, section_configs: List[ExtractionConfig]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract data for a specific section\"\"\"\n",
        "        section_data = {}\n",
        "\n",
        "        for config in section_configs:\n",
        "            try:\n",
        "                if config.extraction_type == \"single\":\n",
        "                    value = self.extract_single_value(text, config.patterns)\n",
        "                elif config.extraction_type == \"multiple\":\n",
        "                    value = self.extract_multiple_values(text, config.patterns)\n",
        "                else:\n",
        "                    value = config.default_value\n",
        "\n",
        "                # Set default if empty and required\n",
        "                if not value and config.required:\n",
        "                    value = config.default_value\n",
        "\n",
        "                section_data[config.field_name] = value\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error extracting {config.field_name}: {e}\")\n",
        "                section_data[config.field_name] = config.default_value\n",
        "\n",
        "        return section_data\n",
        "\n",
        "    def extract_references(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"Extract bibliography/references\"\"\"\n",
        "        references = []\n",
        "\n",
        "        # Find references section\n",
        "        ref_patterns = [\n",
        "            r\"References.*?:\\s*(.*?)(?=Appendix|$)\",\n",
        "            r\"Bibliography.*?:\\s*(.*?)(?=Appendix|$)\",\n",
        "            r\"Works Cited.*?:\\s*(.*?)(?=Appendix|$)\"\n",
        "        ]\n",
        "\n",
        "        ref_text = \"\"\n",
        "        for pattern in ref_patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                ref_text = match.group(1)\n",
        "                break\n",
        "\n",
        "        if ref_text:\n",
        "            # Split references (simple approach)\n",
        "            ref_lines = re.split(r'\\n(?=[A-Z])', ref_text)\n",
        "\n",
        "            for i, ref_line in enumerate(ref_lines):\n",
        "                if len(ref_line.strip()) > 50:  # Filter out short lines\n",
        "                    references.append({\n",
        "                        \"id\": i + 1,\n",
        "                        \"citation\": self._clean_text(ref_line),\n",
        "                        \"type\": \"unknown\"\n",
        "                    })\n",
        "\n",
        "        return references\n",
        "\n",
        "    def process_document(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Main processing function that extracts all data into JSON structure\"\"\"\n",
        "        logger.info(f\"Processing {self.document_type} document: {file_path}\")\n",
        "\n",
        "        # Extract text\n",
        "        full_text = self.extract_text_from_pdf(file_path)\n",
        "\n",
        "        # Initialize result structure\n",
        "        result = {\n",
        "            \"document_info\": {\n",
        "                \"file_path\": str(file_path),\n",
        "                \"document_type\": self.document_type,\n",
        "                \"extraction_timestamp\": pd.Timestamp.now().isoformat()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Extract data for each section\n",
        "        for section_name, section_configs in self.extraction_configs.items():\n",
        "            logger.info(f\"Extracting {section_name} data...\")\n",
        "            section_data = self.extract_section_data(full_text, section_configs)\n",
        "            result[section_name] = section_data\n",
        "\n",
        "        # Extract references\n",
        "        logger.info(\"Extracting references...\")\n",
        "        result[\"references\"] = self.extract_references(full_text)\n",
        "\n",
        "        # Add full text if needed (optional)\n",
        "        # result[\"full_text\"] = full_text\n",
        "\n",
        "        return result\n",
        "\n",
        "    def save_json(self, data: Dict[str, Any], output_path: str):\n",
        "        \"\"\"Save extracted data to JSON file\"\"\"\n",
        "        output_path = Path(output_path)\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"Data saved to {output_path}\")\n",
        "\n",
        "    def validate_extracted_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Validate and add quality metrics to extracted data\"\"\"\n",
        "        validation_results = {\n",
        "            \"completeness_score\": 0,\n",
        "            \"missing_required_fields\": [],\n",
        "            \"data_quality_issues\": []\n",
        "        }\n",
        "\n",
        "        total_fields = 0\n",
        "        filled_fields = 0\n",
        "\n",
        "        for section_name, section_data in data.items():\n",
        "            if section_name == \"document_info\":\n",
        "                continue\n",
        "\n",
        "            if isinstance(section_data, dict):\n",
        "                for field_name, field_value in section_data.items():\n",
        "                    total_fields += 1\n",
        "                    if field_value and field_value != \"\":\n",
        "                        filled_fields += 1\n",
        "                    else:\n",
        "                        validation_results[\"missing_required_fields\"].append(f\"{section_name}.{field_name}\")\n",
        "\n",
        "        validation_results[\"completeness_score\"] = filled_fields / total_fields if total_fields > 0 else 0\n",
        "\n",
        "        return validation_results\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "\n",
        "    # Initialize extractor for NSF grant\n",
        "    extractor = JSONDocumentExtractor(document_type=\"nsf_grant\")\n",
        "\n",
        "    try:\n",
        "        # Process document\n",
        "        pdf_path = \"/content/1-k08-ai155816-01a1-aladra-application-508.pdf\"  # Replace with your PDF path\n",
        "        extracted_data = extractor.process_document(pdf_path)\n",
        "\n",
        "        # Validate data\n",
        "        validation_results = extractor.validate_extracted_data(extracted_data)\n",
        "        extracted_data[\"validation\"] = validation_results\n",
        "\n",
        "        # Save to JSON\n",
        "        extractor.save_json(extracted_data, \"extracted_nsf_grant5.json\")\n",
        "\n",
        "        print(\"Extraction completed successfully!\")\n",
        "        print(f\"Completeness score: {validation_results['completeness_score']:.2%}\")\n",
        "        print(f\"Total sections extracted: {len(extracted_data) - 2}\")  # Excluding document_info and validation\n",
        "\n",
        "        # Print summary\n",
        "        for section_name, section_data in extracted_data.items():\n",
        "            if section_name not in [\"document_info\", \"validation\"]:\n",
        "                if isinstance(section_data, dict):\n",
        "                    filled_fields = sum(1 for v in section_data.values() if v and v != \"\")\n",
        "                    print(f\"{section_name}: {filled_fields}/{len(section_data)} fields extracted\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing document: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoTygPG_qbFw",
        "outputId": "180260c1-98ca-43b3-fa51-b187046a174b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed successfully!\n",
            "Completeness score: 66.67%\n",
            "Total sections extracted: 7\n",
            "administrative_info: 5/8 fields extracted\n",
            "budget: 5/5 fields extracted\n",
            "project_description: 2/4 fields extracted\n",
            "research_plan: 4/4 fields extracted\n",
            "compliance: 0/3 fields extracted\n",
            "personnel: 2/3 fields extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load \"/content/extracted_nsf_grant1.json\" , /content/extracted_nsf_grant2.json' .... /content/extracted_nsf_grant5.json and give me a list of all of the columns or keys of the json, whatever it is called.\n",
        "\n",
        "file_paths = [f'/content/extracted_nsf_grant{i}.json' for i in range(1, 6)]\n",
        "\n",
        "all_keys = set()\n",
        "\n",
        "for file_path in file_paths:\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Recursively get all keys from nested dictionaries\n",
        "        def get_keys(obj, current_path=\"\"):\n",
        "            if isinstance(obj, dict):\n",
        "                for key, value in obj.items():\n",
        "                    new_path = f\"{current_path}.{key}\" if current_path else key\n",
        "                    all_keys.add(new_path)\n",
        "                    get_keys(value, new_path)\n",
        "            elif isinstance(obj, list):\n",
        "                # Optionally add keys for list items if they are dicts\n",
        "                for i, item in enumerate(obj):\n",
        "                    # new_path = f\"{current_path}[{i}]\" # You can add array index if needed\n",
        "                    get_keys(item, current_path)\n",
        "\n",
        "        get_keys(data)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: File not found at {file_path}\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred processing {file_path}: {e}\")\n",
        "\n",
        "print(\"List of all unique keys across all JSON files:\")\n",
        "for key in sorted(list(all_keys)):\n",
        "    print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDsBWIxbt47V",
        "outputId": "7ecbcb55-e0cf-4813-83fd-7db26495cfcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of all unique keys across all JSON files:\n",
            "administrative_info\n",
            "administrative_info.institution\n",
            "administrative_info.nsf_program\n",
            "administrative_info.pi_email\n",
            "administrative_info.pi_name\n",
            "administrative_info.proposal_id\n",
            "administrative_info.proposal_title\n",
            "administrative_info.requested_amount\n",
            "administrative_info.submission_date\n",
            "budget\n",
            "budget.equipment_costs\n",
            "budget.indirect_costs\n",
            "budget.personnel_costs\n",
            "budget.total_budget\n",
            "budget.travel_costs\n",
            "compliance\n",
            "compliance.biohazards\n",
            "compliance.human_subjects\n",
            "compliance.vertebrate_animals\n",
            "document_info\n",
            "document_info.document_type\n",
            "document_info.extraction_timestamp\n",
            "document_info.file_path\n",
            "personnel\n",
            "personnel.co_investigators\n",
            "personnel.collaborators\n",
            "personnel.senior_personnel\n",
            "project_description\n",
            "project_description.broader_impacts\n",
            "project_description.intellectual_merit\n",
            "project_description.keywords\n",
            "project_description.overview\n",
            "references\n",
            "references.citation\n",
            "references.id\n",
            "references.type\n",
            "research_plan\n",
            "research_plan.innovation\n",
            "research_plan.methodology\n",
            "research_plan.objectives\n",
            "research_plan.timeline\n",
            "validation\n",
            "validation.completeness_score\n",
            "validation.data_quality_issues\n",
            "validation.missing_required_fields\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import PyPDF2\n",
        "from typing import Dict, Any, List, Optional, Union\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GeneralizedNIHSummaryParser:\n",
        "    \"\"\"Generalized parser for various types of NIH Grant Summary Statements\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.data = {}\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF file\"\"\"\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                for page_num, page in enumerate(pdf_reader.pages):\n",
        "                    try:\n",
        "                        page_text = page.extract_text()\n",
        "                        text += f\"\\n{page_text}\\n\"\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Could not extract text from page {page_num + 1}: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading PDF: {e}\")\n",
        "            raise\n",
        "        return text\n",
        "\n",
        "    def detect_grant_type(self, text: str) -> str:\n",
        "        \"\"\"Detect the type of NIH grant from the document\"\"\"\n",
        "        # Look for common grant type indicators\n",
        "        if re.search(r'\\bR43\\b|\\bR44\\b|SBIR|Small Business', text, re.IGNORECASE):\n",
        "            return \"SBIR\"\n",
        "        elif re.search(r'\\bK\\d+\\b.*?(Career|Fellowship)', text, re.IGNORECASE):\n",
        "            return \"Career_Development\"\n",
        "        elif re.search(r'\\bR01\\b|\\bR21\\b|\\bR03\\b', text, re.IGNORECASE):\n",
        "            return \"Research_Grant\"\n",
        "        elif re.search(r'\\bU\\d+\\b', text, re.IGNORECASE):\n",
        "            return \"Cooperative_Agreement\"\n",
        "        elif re.search(r'\\bP\\d+\\b', text, re.IGNORECASE):\n",
        "            return \"Program_Project\"\n",
        "        else:\n",
        "            return \"Unknown\"\n",
        "\n",
        "    def extract_administrative_info(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract basic administrative information\"\"\"\n",
        "        admin_info = {}\n",
        "\n",
        "        # Application number - more flexible pattern\n",
        "        app_patterns = [\n",
        "            r'Application Number:\\s*([A-Z0-9\\s\\-]+)',\n",
        "            r'(\\d+\\s+[A-Z]\\d+\\s+[A-Z]{2}\\d+\\-\\d+)',\n",
        "            r'Grant Number:\\s*([A-Z0-9\\s\\-]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in app_patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                admin_info['application_number'] = match.group(1).strip()\n",
        "                break\n",
        "\n",
        "        # Principal Investigator(s) - handle multiple formats\n",
        "        pi_patterns = [\n",
        "            r'Principal Investigator[^\\n]*?:\\s*([A-Z\\-,\\s]+?)(?:\\(Contact\\)|Applicant|Review)',\n",
        "            r'Principal Investigators[^\\n]*?:\\s*(.*?)(?=Applicant Organization|Review Group)',\n",
        "            r'PI.*?:\\s*([A-Z\\-,\\s]+?)(?:\\n|Applicant)'\n",
        "        ]\n",
        "\n",
        "        for pattern in pi_patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                pi_text = match.group(1).strip()\n",
        "                # Clean up the PI text\n",
        "                pi_text = re.sub(r'\\(Contact\\)', '', pi_text)\n",
        "                pi_text = re.sub(r'\\s+', ' ', pi_text)\n",
        "                admin_info['principal_investigator'] = pi_text\n",
        "                break\n",
        "\n",
        "        # Extract individual PIs if listed alphabetically\n",
        "        pi_list_match = re.search(r'Principal Investigators \\(Listed Alphabetically\\):\\s*(.*?)(?=Applicant Organization)', text, re.DOTALL)\n",
        "        if pi_list_match:\n",
        "            pi_list_text = pi_list_match.group(1)\n",
        "            # Split by lines and clean\n",
        "            pis = [line.strip() for line in pi_list_text.split('\\n') if line.strip() and not re.match(r'^\\d+', line)]\n",
        "            admin_info['principal_investigators_list'] = pis\n",
        "\n",
        "        # Organization\n",
        "        org_patterns = [\n",
        "            r'Applicant Organization:\\s*([^\\n]+)',\n",
        "            r'Institution:\\s*([^\\n]+)',\n",
        "            r'Organization:\\s*([^\\n]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in org_patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                admin_info['applicant_organization'] = match.group(1).strip()\n",
        "                break\n",
        "\n",
        "        # Review Group\n",
        "        review_patterns = [\n",
        "            r'Review Group:\\s*([^\\n]+(?:\\n[^\\n]+)*?)(?=Meeting Date|Center for)',\n",
        "            r'Study Section:\\s*([^\\n]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in review_patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                review_text = match.group(1).strip()\n",
        "                review_text = re.sub(r'\\s+', ' ', review_text)\n",
        "                admin_info['review_group'] = review_text\n",
        "                break\n",
        "\n",
        "        # Meeting Date\n",
        "        meeting_patterns = [\n",
        "            r'Meeting Date:\\s*([0-9/]+)',\n",
        "            r'Review Date:\\s*([0-9/]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in meeting_patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                admin_info['meeting_date'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        # Council\n",
        "        council_match = re.search(r'Council:\\s*([A-Z0-9\\s]+)', text)\n",
        "        admin_info['council'] = council_match.group(1).strip() if council_match else \"\"\n",
        "\n",
        "        # RFA/PA\n",
        "        rfa_match = re.search(r'RFA/PA:\\s*([A-Z0-9\\-]+)', text)\n",
        "        admin_info['rfa_pa'] = rfa_match.group(1) if rfa_match else \"\"\n",
        "\n",
        "        # Requested Start Date\n",
        "        start_patterns = [\n",
        "            r'Requested Start:\\s*([0-9/]+)',\n",
        "            r'Start Date:\\s*([0-9/]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in start_patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                admin_info['requested_start'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        # Project Title - more flexible extraction\n",
        "        title_patterns = [\n",
        "            r'Project Title:\\s*([^\\n]+(?:\\n[^\\n]+)*?)(?=SRG Action:|Impact Score:|Next Steps:|Human Subjects)',\n",
        "            r'Title:\\s*([^\\n]+(?:\\n[^\\n]+)*?)(?=SRG Action:|Impact Score:|PI:|Principal)'\n",
        "        ]\n",
        "\n",
        "        for pattern in title_patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                title = match.group(1).strip()\n",
        "                title = re.sub(r'\\s+', ' ', title)\n",
        "                admin_info['project_title'] = title\n",
        "                break\n",
        "\n",
        "        # Impact Score - handle both numeric and text formats\n",
        "        impact_patterns = [\n",
        "            r'Impact Score:\\s*(\\d+)',\n",
        "            r'Impact Score:\\s*([^\\n]+)',\n",
        "            r'Priority Score:\\s*(\\d+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in impact_patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                admin_info['impact_score'] = match.group(1).strip()\n",
        "                break\n",
        "\n",
        "        # Human and Animal Subjects\n",
        "        human_subjects_match = re.search(r'Human Subjects:\\s*([^\\n]+)', text)\n",
        "        admin_info['human_subjects'] = human_subjects_match.group(1).strip() if human_subjects_match else \"\"\n",
        "\n",
        "        animal_subjects_match = re.search(r'Animal Subjects:\\s*([^\\n]+)', text)\n",
        "        admin_info['animal_subjects'] = animal_subjects_match.group(1).strip() if animal_subjects_match else \"\"\n",
        "\n",
        "        # Grant type detection\n",
        "        admin_info['grant_type'] = self.detect_grant_type(text)\n",
        "\n",
        "        return admin_info\n",
        "\n",
        "    def extract_budget_info(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract budget information - handles various formats\"\"\"\n",
        "        budget_info = {}\n",
        "\n",
        "        # Look for budget table patterns\n",
        "        budget_patterns = [\n",
        "            r'Project\\s+Direct Costs\\s+Estimated\\s+Year\\s+Requested\\s+Total Cost\\s+((?:\\d+.*?\\n.*?\\n.*?\\n)+)',\n",
        "            r'Budget Year\\s+Direct Costs\\s+Total Costs?\\s+((?:\\d+.*?\\n.*?\\n)+)',\n",
        "            r'Year\\s+\\d+.*?(\\$[\\d,]+).*?(\\$[\\d,]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in budget_patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                budget_text = match.group(1) if len(match.groups()) > 0 else match.group(0)\n",
        "                years = re.findall(r'Year\\s+(\\d+)|^(\\d+)\\s', budget_text, re.MULTILINE)\n",
        "                if years:\n",
        "                    budget_info['project_years'] = [y[0] or y[1] for y in years]\n",
        "                break\n",
        "\n",
        "        # Total budget - multiple patterns\n",
        "        total_patterns = [\n",
        "            r'TOTAL\\s+([0-9,\\$]+)\\s+([0-9,\\$]+)',\n",
        "            r'Total.*?(\\$[\\d,]+).*?(\\$[\\d,]+)',\n",
        "            r'Direct Costs.*?(\\$[\\d,]+)',\n",
        "            r'Total Cost.*?(\\$[\\d,]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in total_patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                if len(match.groups()) >= 2:\n",
        "                    budget_info['total_direct_costs'] = match.group(1)\n",
        "                    budget_info['total_estimated_cost'] = match.group(2)\n",
        "                else:\n",
        "                    budget_info['total_amount'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        return budget_info\n",
        "\n",
        "    def extract_project_description(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract project description and related sections\"\"\"\n",
        "        description_info = {}\n",
        "\n",
        "        # Main description - multiple possible headers\n",
        "        desc_patterns = [\n",
        "            r'DESCRIPTION \\(provided by applicant\\):\\s*(.*?)(?=PUBLIC HEALTH RELEVANCE:|CRITIQUE|$)',\n",
        "            r'Project Description:\\s*(.*?)(?=PUBLIC HEALTH RELEVANCE:|CRITIQUE|$)',\n",
        "            r'Abstract:\\s*(.*?)(?=PUBLIC HEALTH RELEVANCE:|CRITIQUE|$)'\n",
        "        ]\n",
        "\n",
        "        for pattern in desc_patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                description_info['description'] = self._clean_text(match.group(1))\n",
        "                break\n",
        "\n",
        "        # Public Health Relevance\n",
        "        relevance_patterns = [\n",
        "            r'PUBLIC HEALTH RELEVANCE:\\s*(.*?)(?=CRITIQUE|RESUME|$)',\n",
        "            r'Public Health Relevance Statement:\\s*(.*?)(?=CRITIQUE|RESUME|$)',\n",
        "            r'Relevance:\\s*(.*?)(?=CRITIQUE|RESUME|$)'\n",
        "        ]\n",
        "\n",
        "        for pattern in relevance_patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                description_info['public_health_relevance'] = self._clean_text(match.group(1))\n",
        "                break\n",
        "\n",
        "        return description_info\n",
        "\n",
        "    def extract_critique_sections(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extract all critique sections - adaptable to different formats\"\"\"\n",
        "        critiques = []\n",
        "\n",
        "        # First try the standard pattern with colon\n",
        "        critique_pattern = r'CRITIQUE (\\d+):(.*?)(?=CRITIQUE \\d+:|THE FOLLOWING|COMMITTEE BUDGET|$)'\n",
        "        critique_matches = list(re.finditer(critique_pattern, text, re.DOTALL))\n",
        "\n",
        "        # If no matches, try without colon (like your document)\n",
        "        if not critique_matches:\n",
        "            critique_pattern = r'CRITIQUE (\\d+)\\s*\\n(.*?)(?=CRITIQUE \\d+|THE FOLLOWING|COMMITTEE BUDGET|$)'\n",
        "            critique_matches = list(re.finditer(critique_pattern, text, re.DOTALL))\n",
        "\n",
        "        # If still no matches, try finding them by looking for score patterns\n",
        "        if not critique_matches:\n",
        "            # Look for patterns like \"Significance: 2\" which indicate start of critiques\n",
        "            score_sections = list(re.finditer(r'(Significance:\\s*\\d+.*?)(?=Significance:\\s*\\d+|THE FOLLOWING|COMMITTEE BUDGET|$)', text, re.DOTALL))\n",
        "            for i, match in enumerate(score_sections):\n",
        "                critique_matches.append(type('obj', (object,), {\n",
        "                    'group': lambda self, x: str(i+1) if x == 1 else match.group(1)\n",
        "                })())\n",
        "\n",
        "        for match in critique_matches:\n",
        "            critique_num = match.group(1)\n",
        "            critique_text = match.group(2)\n",
        "\n",
        "            # Extract scores if present\n",
        "            scores = self._extract_critique_scores(critique_text)\n",
        "\n",
        "            critique_data = {\n",
        "                'critique_number': critique_num,\n",
        "                'scores': scores,\n",
        "                'overall_impact': self._extract_section_content(critique_text, 'Overall Impact'),\n",
        "                'significance': self._extract_evaluation_section(critique_text, 'Significance'),\n",
        "                'investigator': self._extract_evaluation_section(critique_text, 'Investigator'),\n",
        "                'innovation': self._extract_evaluation_section(critique_text, 'Innovation'),\n",
        "                'approach': self._extract_evaluation_section(critique_text, 'Approach'),\n",
        "                'environment': self._extract_evaluation_section(critique_text, 'Environment'),\n",
        "                # Additional sections that might be present\n",
        "                'candidate': self._extract_evaluation_section(critique_text, 'Candidate'),\n",
        "                'career_development': self._extract_evaluation_section(critique_text, 'Career Development'),\n",
        "                'research_plan': self._extract_evaluation_section(critique_text, 'Research Plan'),\n",
        "                'mentor': self._extract_evaluation_section(critique_text, 'Mentor'),\n",
        "                'training_plan': self._extract_evaluation_section(critique_text, 'Training Plan')\n",
        "            }\n",
        "\n",
        "            # Extract compliance sections\n",
        "            critique_data['compliance'] = self._extract_compliance_from_critique(critique_text)\n",
        "\n",
        "            # Remove empty sections\n",
        "            critique_data = {k: v for k, v in critique_data.items() if v}\n",
        "\n",
        "            critiques.append(critique_data)\n",
        "\n",
        "        return critiques\n",
        "\n",
        "    def _extract_critique_scores(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract numerical scores from critique\"\"\"\n",
        "        scores = {}\n",
        "\n",
        "        score_patterns = [\n",
        "            (r'Significance:\\s*(\\d+)', 'significance'),\n",
        "            (r'Investigator(?:\\(s\\))?:\\s*(\\d+)', 'investigator'),\n",
        "            (r'Innovation:\\s*(\\d+)', 'innovation'),\n",
        "            (r'Approach:\\s*(\\d+)', 'approach'),\n",
        "            (r'Environment:\\s*(\\d+)', 'environment'),\n",
        "            (r'Candidate:\\s*(\\d+)', 'candidate'),\n",
        "            (r'Career Development.*?:\\s*(\\d+)', 'career_development'),\n",
        "            (r'Research Plan:\\s*(\\d+)', 'research_plan'),\n",
        "            (r'Mentor.*?:\\s*(\\d+)', 'mentor')\n",
        "        ]\n",
        "\n",
        "        for pattern, key in score_patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                scores[key] = match.group(1)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def _extract_evaluation_section(self, text: str, section_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract strengths and weaknesses for a specific evaluation section\"\"\"\n",
        "        section_data = {}\n",
        "\n",
        "        # Multiple patterns to try for section headers\n",
        "        section_patterns = [\n",
        "            rf'\\d+\\.\\s*{section_name}.*?:(.*?)(?=\\d+\\.\\s*\\w+.*?:|Protections for Human|Vertebrate Animals|Budget and Period|$)',\n",
        "            rf'{section_name}:\\s*(.*?)(?=\\d+\\.\\s*\\w+.*?:|Protections for Human|Vertebrate Animals|Budget and Period|$)',\n",
        "            rf'\\d+\\.\\s*{section_name}\\s*\\n(.*?)(?=\\d+\\.\\s*\\w+|Protections for Human|Vertebrate Animals|Budget and Period|$)'\n",
        "        ]\n",
        "\n",
        "        section_match = None\n",
        "        for pattern in section_patterns:\n",
        "            section_match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "            if section_match:\n",
        "                break\n",
        "\n",
        "        if section_match:\n",
        "            section_text = section_match.group(1)\n",
        "\n",
        "            # Extract strengths - handle both \"Strengths\" and bullet points directly\n",
        "            strengths_patterns = [\n",
        "                r'Strengths?\\s*(.*?)(?=Weaknesses?|\\d+\\.|$)',\n",
        "                r'Strengths?\\s*\\n(.*?)(?=Weaknesses?|\\d+\\.|$)'\n",
        "            ]\n",
        "\n",
        "            for pattern in strengths_patterns:\n",
        "                strengths_match = re.search(pattern, section_text, re.DOTALL)\n",
        "                if strengths_match:\n",
        "                    strengths_text = strengths_match.group(1)\n",
        "                    section_data['strengths'] = self._extract_bullet_points(strengths_text)\n",
        "                    break\n",
        "\n",
        "            # Extract weaknesses\n",
        "            weaknesses_patterns = [\n",
        "                r'Weaknesses?\\s*(.*?)(?=Strengths?|\\d+\\.|$)',\n",
        "                r'Weaknesses?\\s*\\n(.*?)(?=Strengths?|\\d+\\.|$)'\n",
        "            ]\n",
        "\n",
        "            for pattern in weaknesses_patterns:\n",
        "                weaknesses_match = re.search(pattern, section_text, re.DOTALL)\n",
        "                if weaknesses_match:\n",
        "                    weaknesses_text = weaknesses_match.group(1)\n",
        "                    section_data['weaknesses'] = self._extract_bullet_points(weaknesses_text)\n",
        "                    break\n",
        "\n",
        "        return section_data\n",
        "\n",
        "    def _extract_bullet_points(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract bullet points or numbered items from text\"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # Split by various bullet point indicators\n",
        "        items = re.split(r'\\n\\s*[•·▪▫]\\s*|\\n\\s*\\d+\\.\\s*|\\n\\s*[a-z]\\)\\s*|\\n\\s*-\\s+', text)\n",
        "\n",
        "        # Also try splitting by line breaks if no bullet points found\n",
        "        if len(items) <= 1:\n",
        "            # Look for sentences that start new thoughts\n",
        "            items = re.split(r'\\n\\s*(?=[A-Z])', text)\n",
        "\n",
        "        # Clean and filter items\n",
        "        cleaned_items = []\n",
        "        for item in items:\n",
        "            cleaned = self._clean_text(item)\n",
        "            # More flexible filtering - accept shorter meaningful statements\n",
        "            if cleaned and len(cleaned) > 5 and not re.match(r'^\\s*(None|N/A|Not applicable)', cleaned, re.IGNORECASE):\n",
        "                cleaned_items.append(cleaned)\n",
        "\n",
        "        return cleaned_items\n",
        "\n",
        "    def _extract_section_content(self, text: str, section_name: str) -> str:\n",
        "        \"\"\"Extract content from a named section\"\"\"\n",
        "        pattern = rf'{section_name}:\\s*(.*?)(?=\\d+\\.|Protections for Human|Vertebrate Animals|Budget and Period|$)'\n",
        "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "        return self._clean_text(match.group(1)) if match else \"\"\n",
        "\n",
        "    def _extract_compliance_from_critique(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract compliance information from critique section\"\"\"\n",
        "        compliance = {}\n",
        "\n",
        "        compliance_patterns = [\n",
        "            (r'Protections for Human Subjects:\\s*([^\\n]+)', 'human_subjects'),\n",
        "            (r'Vertebrate Animals:\\s*([^\\n]+)', 'vertebrate_animals'),\n",
        "            (r'Biohazards:\\s*([^\\n]+)', 'biohazards'),\n",
        "            (r'Authentication of Key Biological.*?:\\s*([^\\n]+)', 'resource_authentication'),\n",
        "            (r'Budget and Period of Support:\\s*([^\\n]+)', 'budget_recommendation')\n",
        "        ]\n",
        "\n",
        "        for pattern, key in compliance_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                compliance[key] = match.group(1).strip()\n",
        "\n",
        "        return compliance\n",
        "\n",
        "    def extract_resume_summary(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract resume and summary of discussion\"\"\"\n",
        "        resume_info = {}\n",
        "\n",
        "        # Multiple possible patterns for summary\n",
        "        summary_patterns = [\n",
        "            r'RESUME AND SUMMARY OF DISCUSSION:\\s*(.*?)(?=DESCRIPTION|CRITIQUE|$)',\n",
        "            r'Summary of Discussion:\\s*(.*?)(?=DESCRIPTION|CRITIQUE|$)',\n",
        "            r'Review Summary:\\s*(.*?)(?=DESCRIPTION|CRITIQUE|$)'\n",
        "        ]\n",
        "\n",
        "        for pattern in summary_patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                resume_info['summary_of_discussion'] = self._clean_text(match.group(1))\n",
        "                break\n",
        "\n",
        "        return resume_info\n",
        "\n",
        "    def extract_committee_notes(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract scientific review officer's notes and committee recommendations\"\"\"\n",
        "        notes_info = {}\n",
        "\n",
        "        # Scientific Review Officer's Notes\n",
        "        sro_match = re.search(r'SCIENTIFIC REVIEW OFFICER\\'S NOTES:\\s*(.*?)(?=COMMITTEE BUDGET|$)', text, re.DOTALL)\n",
        "        if sro_match:\n",
        "            notes_info['scientific_review_officer_notes'] = self._clean_text(sro_match.group(1))\n",
        "\n",
        "        # Committee Budget Recommendations\n",
        "        budget_rec_patterns = [\n",
        "            r'COMMITTEE BUDGET RECOMMENDATIONS:\\s*(.*?)(?=Footnotes|$)',\n",
        "            r'Budget Recommendations:\\s*(.*?)(?=Footnotes|$)'\n",
        "        ]\n",
        "\n",
        "        for pattern in budget_rec_patterns:\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                notes_info['committee_budget_recommendations'] = self._clean_text(match.group(1))\n",
        "                break\n",
        "\n",
        "        return notes_info\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # Remove common page headers/footers patterns\n",
        "        text = re.sub(r'\\d+\\s+[A-Z]\\d+\\s+[A-Z]{2}\\d+\\-\\d+.*?ZRG\\d+\\s+[A-Z\\-]+\\s*\\(\\d+\\)', '', text)\n",
        "        text = re.sub(r'[A-Z\\-]+,\\s*[A-Z]\\s*$', '', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def process_summary_statement(self, pdf_path: str, debug: bool = False) -> Dict[str, Any]:\n",
        "        \"\"\"Main processing function - works with various NIH grant types\"\"\"\n",
        "        logger.info(f\"Processing NIH Summary Statement: {pdf_path}\")\n",
        "\n",
        "        # Extract text\n",
        "        full_text = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Debug: Print text sections if requested\n",
        "        if debug:\n",
        "            print(\"=== DEBUG: Looking for CRITIQUE sections ===\")\n",
        "            critique_positions = []\n",
        "            for match in re.finditer(r'CRITIQUE \\d+', full_text):\n",
        "                start_pos = match.start()\n",
        "                end_pos = min(start_pos + 200, len(full_text))\n",
        "                print(f\"Found CRITIQUE at position {start_pos}: {full_text[start_pos:end_pos]}...\")\n",
        "                critique_positions.append(start_pos)\n",
        "\n",
        "            if critique_positions:\n",
        "                print(f\"\\nFound {len(critique_positions)} CRITIQUE sections\")\n",
        "                # Print a sample of the first critique\n",
        "                if len(critique_positions) > 0:\n",
        "                    start = critique_positions[0]\n",
        "                    end = critique_positions[1] if len(critique_positions) > 1 else min(start + 1000, len(full_text))\n",
        "                    print(f\"\\n=== SAMPLE OF FIRST CRITIQUE ===\")\n",
        "                    print(full_text[start:end])\n",
        "                    print(\"=== END SAMPLE ===\\n\")\n",
        "            else:\n",
        "                print(\"No CRITIQUE sections found!\")\n",
        "                # Look for other patterns\n",
        "                print(\"\\n=== Looking for alternative patterns ===\")\n",
        "                for pattern in ['Significance:', 'Investigator', 'Innovation:', 'Approach:', 'Environment:']:\n",
        "                    matches = list(re.finditer(pattern, full_text))\n",
        "                    print(f\"Found {len(matches)} instances of '{pattern}'\")\n",
        "\n",
        "        # Detect grant type first\n",
        "        grant_type = self.detect_grant_type(full_text)\n",
        "        logger.info(f\"Detected grant type: {grant_type}\")\n",
        "\n",
        "        # Extract all sections\n",
        "        result = {\n",
        "            \"document_info\": {\n",
        "                \"file_path\": str(pdf_path),\n",
        "                \"document_type\": \"nih_summary_statement\",\n",
        "                \"grant_type\": grant_type,\n",
        "                \"extraction_timestamp\": pd.Timestamp.now().isoformat()\n",
        "            },\n",
        "            \"administrative_info\": self.extract_administrative_info(full_text),\n",
        "            \"budget_info\": self.extract_budget_info(full_text),\n",
        "            \"project_description\": self.extract_project_description(full_text),\n",
        "            \"resume_and_summary\": self.extract_resume_summary(full_text),\n",
        "            \"critiques\": self.extract_critique_sections(full_text),\n",
        "            \"committee_notes\": self.extract_committee_notes(full_text)\n",
        "        }\n",
        "\n",
        "        if debug:\n",
        "            print(f\"\\n=== EXTRACTION RESULTS ===\")\n",
        "            print(f\"Found {len(result['critiques'])} critiques\")\n",
        "            for i, critique in enumerate(result['critiques']):\n",
        "                print(f\"Critique {i+1}: {list(critique.keys())}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def save_json(self, data: Dict[str, Any], output_path: str):\n",
        "        \"\"\"Save extracted data to JSON file\"\"\"\n",
        "        output_path = Path(output_path)\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"Data saved to {output_path}\")\n",
        "\n",
        "    def create_summary_report(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Create a summary report of the extraction\"\"\"\n",
        "        admin_info = data.get(\"administrative_info\", {})\n",
        "\n",
        "        summary = {\n",
        "            \"extraction_summary\": {\n",
        "                \"grant_type\": data.get(\"document_info\", {}).get(\"grant_type\", \"\"),\n",
        "                \"application_number\": admin_info.get(\"application_number\", \"\"),\n",
        "                \"principal_investigator\": admin_info.get(\"principal_investigator\", \"\"),\n",
        "                \"project_title\": admin_info.get(\"project_title\", \"\"),\n",
        "                \"impact_score\": admin_info.get(\"impact_score\", \"\"),\n",
        "                \"review_group\": admin_info.get(\"review_group\", \"\"),\n",
        "                \"organization\": admin_info.get(\"applicant_organization\", \"\"),\n",
        "                \"number_of_critiques\": len(data.get(\"critiques\", [])),\n",
        "                \"has_description\": bool(data.get(\"project_description\", {}).get(\"description\", \"\")),\n",
        "                \"has_public_health_relevance\": bool(data.get(\"project_description\", {}).get(\"public_health_relevance\", \"\")),\n",
        "                \"has_committee_notes\": bool(data.get(\"committee_notes\", {})),\n",
        "                \"sections_extracted\": list(data.keys())\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def save_summary_csv(self, data: Dict[str, Any], output_path: str):\n",
        "        \"\"\"Save a CSV summary of key information\"\"\"\n",
        "        summary = self.create_summary_report(data)\n",
        "\n",
        "        # Flatten the data for CSV\n",
        "        flat_data = {}\n",
        "        flat_data.update(data.get(\"administrative_info\", {}))\n",
        "        flat_data.update(summary.get(\"extraction_summary\", {}))\n",
        "\n",
        "        # Add critique summary\n",
        "        critiques = data.get(\"critiques\", [])\n",
        "        for i, critique in enumerate(critiques):\n",
        "            if 'overall_impact' in critique:\n",
        "                flat_data[f\"critique_{i+1}_overall_impact\"] = critique[\"overall_impact\"][:200]\n",
        "            if 'scores' in critique:\n",
        "                for score_type, score_value in critique['scores'].items():\n",
        "                    flat_data[f\"critique_{i+1}_{score_type}_score\"] = score_value\n",
        "\n",
        "        # Create DataFrame and save\n",
        "        df = pd.DataFrame([flat_data])\n",
        "        df.to_csv(output_path, index=False)\n",
        "        logger.info(f\"Summary CSV saved to {output_path}\")\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    parser = GeneralizedNIHSummaryParser()\n",
        "\n",
        "    try:\n",
        "        # Process the summary statement\n",
        "        pdf_path = \"/content/R43-Summary-Statement_ MacLeod-1R43AI145704-01.pdf\"  # Replace with your PDF path\n",
        "        extracted_data = parser.process_summary_statement(pdf_path)\n",
        "\n",
        "        # Save to JSON\n",
        "        parser.save_json(extracted_data, \"nih_summary_extracted4.json\")\n",
        "\n",
        "        # Save summary CSV\n",
        "        parser.save_summary_csv(extracted_data, \"nih_summary_key_info.csv\")\n",
        "\n",
        "        # Print summary\n",
        "        summary = parser.create_summary_report(extracted_data)\n",
        "        print(\"Extraction Summary:\")\n",
        "        print(f\"Grant Type: {summary['extraction_summary']['grant_type']}\")\n",
        "        print(f\"Application: {summary['extraction_summary']['application_number']}\")\n",
        "        print(f\"PI: {summary['extraction_summary']['principal_investigator']}\")\n",
        "        print(f\"Impact Score: {summary['extraction_summary']['impact_score']}\")\n",
        "        print(f\"Number of Critiques: {summary['extraction_summary']['number_of_critiques']}\")\n",
        "\n",
        "        # Print critique overview\n",
        "        critiques = extracted_data.get(\"critiques\", [])\n",
        "        for i, critique in enumerate(critiques):\n",
        "            print(f\"\\nCritique {i+1} Overview:\")\n",
        "            if 'scores' in critique:\n",
        "                for score_type, score_value in critique['scores'].items():\n",
        "                    print(f\"  {score_type.title()} Score: {score_value}\")\n",
        "\n",
        "            for section in ['significance', 'investigator', 'innovation', 'approach', 'environment']:\n",
        "                if section in critique and critique[section]:\n",
        "                    strengths = len(critique[section].get('strengths', []))\n",
        "                    weaknesses = len(critique[section].get('weaknesses', []))\n",
        "                    if strengths or weaknesses:\n",
        "                        print(f\"  {section.title()} - Strengths: {strengths}, Weaknesses: {weaknesses}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing summary statement: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmNt3aAAuOhS",
        "outputId": "ee6b542c-1193-4346-adc0-8162a20f04f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction Summary:\n",
            "Grant Type: SBIR\n",
            "Application: 1 R43 AI145704- 01 \n",
            "P\n",
            "PI: \n",
            "Impact Score: \n",
            "Number of Critiques: 1\n",
            "\n",
            "Critique 1 Overview:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced AI Grant Reviewer using Claude API for Google Colab\n",
        "# Make sure to install the required package first\n",
        "\n",
        "# Install required packages\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, Any, List\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import userdata\n",
        "import anthropic\n",
        "\n",
        "class ClaudeGrantReviewer:\n",
        "    \"\"\"\n",
        "    Enhanced AI Grant Reviewer that uses Claude API to analyze grant proposals\n",
        "    and provide detailed critiques similar to NIH summary statements.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str = None):\n",
        "        \"\"\"\n",
        "        Initialize the Claude Grant Reviewer.\n",
        "\n",
        "        Args:\n",
        "            api_key: Anthropic API key. If None, will try to get from Google Colab secrets.\n",
        "        \"\"\"\n",
        "        if api_key is None:\n",
        "            try:\n",
        "                api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "                if not api_key:\n",
        "                    raise ValueError(\"API key is empty\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error getting API key from secrets: {e}\")\n",
        "                raise ValueError(\"API key must be provided either as parameter or stored in Google Colab secrets as 'ANTHROPIC_API_KEY'\")\n",
        "\n",
        "        try:\n",
        "            self.client = anthropic.Anthropic(api_key=api_key)\n",
        "            # Test the connection\n",
        "            self._test_connection()\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Failed to initialize Anthropic client: {e}\")\n",
        "\n",
        "        self.system_prompt = self._get_system_prompt()\n",
        "\n",
        "    def _test_connection(self):\n",
        "        \"\"\"Test the API connection with a simple request.\"\"\"\n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-3-5-sonnet-20241022\",\n",
        "                max_tokens=10,\n",
        "                messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
        "            )\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"API connection test failed: {e}\")\n",
        "\n",
        "    def _get_system_prompt(self) -> str:\n",
        "        \"\"\"Get the enhanced system prompt for Claude.\"\"\"\n",
        "        return \"\"\"<role>\n",
        "You are an expert AI Grant Reviewer, acting as a coach for PhD students. Your persona is that of a seasoned, constructive, and meticulous reviewer from a major funding agency like the NSF or NIH. Your goal is not to be overly harsh, but to provide clear, actionable feedback that will tangibly improve the proposal's chances of getting funded. You will analyze the structured JSON data of a grant proposal provided in the input.\n",
        "</role>\n",
        "\n",
        "<instructions>\n",
        "1. **Analyze Holistically:** Carefully review all sections of the provided JSON input, paying special attention to the `project_description`, `research_plan`, and `budget` objects.\n",
        "\n",
        "2. **Synthesize Key Points:** Identify the core research question, the proposed methods, and the expected outcomes.\n",
        "\n",
        "3. **Structure Your Output:** Your final response must be in Markdown and contain exactly these sections:\n",
        "\n",
        "   **SUMMARY**\n",
        "   - A brief, 2-3 sentence paragraph summarizing the project's goals and approach.\n",
        "\n",
        "   **OVERALL IMPACT AND SIGNIFICANCE**\n",
        "   - Assess the potential impact of the proposed work\n",
        "   - Evaluate the significance to the field and broader scientific community\n",
        "   - Consider the urgency and unmet need addressed\n",
        "\n",
        "   **STRENGTHS**\n",
        "   - A bulleted list of 3-5 key strengths of the proposal\n",
        "   - Focus on elements like intellectual merit, innovation, approach, team expertise, and resources\n",
        "\n",
        "   **WEAKNESSES AND AREAS FOR IMPROVEMENT**\n",
        "   - A numbered list of the most critical weaknesses that need to be addressed\n",
        "   - For each point, first state the issue clearly, then provide a concrete suggestion for how to fix it\n",
        "   - This is the most important section for improvement\n",
        "\n",
        "   **DETAILED CRITIQUE BY CATEGORY**\n",
        "\n",
        "   *Significance and Innovation:*\n",
        "   - Evaluate the importance and novelty of the research\n",
        "   - Assess potential to advance the field\n",
        "\n",
        "   *Approach and Methodology:*\n",
        "   - Review the soundness and feasibility of the research plan\n",
        "   - Evaluate appropriateness of methods and experimental design\n",
        "\n",
        "   *Team and Resources:*\n",
        "   - Assess investigator qualifications and team composition\n",
        "   - Evaluate institutional resources and environment\n",
        "\n",
        "   *Budget and Timeline:*\n",
        "   - Review budget justification and appropriateness\n",
        "   - Assess timeline feasibility\n",
        "\n",
        "   **RECOMMENDATIONS**\n",
        "   - Specific recommendations for strengthening the proposal\n",
        "   - Priority order for addressing weaknesses\n",
        "   - Suggestions for enhancing impact and innovation\n",
        "\n",
        "**Critical Analysis Criteria:**\n",
        "- **Intellectual Merit & Innovation:** Is the idea novel and significant? Does the research plan present a compelling case for advancing knowledge?\n",
        "- **Methodology:** Is the approach sound, well-described, and feasible within the proposed timeline?\n",
        "- **Broader Impacts:** Does the proposal articulate a convincing benefit to society beyond the immediate research?\n",
        "- **Budget Justification:** Do the costs align logically with the proposed research plan?\n",
        "- **Team Expertise:** Does the team have the necessary qualifications and experience?\n",
        "- **Feasibility:** Can the proposed work realistically be completed with the requested resources?\n",
        "\n",
        "You must respond only with the markdown-formatted review following the exact structure specified above.\"\"\"\n",
        "\n",
        "    def validate_json_structure(self, proposal_data: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Validates the structure of the grant proposal JSON data.\n",
        "\n",
        "        Args:\n",
        "            proposal_data: Dictionary containing the proposal data\n",
        "\n",
        "        Returns:\n",
        "            List of validation errors (empty if valid)\n",
        "        \"\"\"\n",
        "        errors = []\n",
        "\n",
        "        # Check if it's a dictionary\n",
        "        if not isinstance(proposal_data, dict):\n",
        "            errors.append(\"Input must be a JSON object/dictionary\")\n",
        "            return errors\n",
        "\n",
        "        # Check required top-level keys\n",
        "        required_keys = ['administrative_info', 'project_description']\n",
        "        for key in required_keys:\n",
        "            if key not in proposal_data:\n",
        "                errors.append(f\"Missing required section: {key}\")\n",
        "\n",
        "        # Check administrative_info structure\n",
        "        if 'administrative_info' in proposal_data:\n",
        "            admin_info = proposal_data['administrative_info']\n",
        "            if isinstance(admin_info, dict):\n",
        "                admin_keys = ['proposal_title', 'pi_name']\n",
        "                for key in admin_keys:\n",
        "                    if key not in admin_info:\n",
        "                        errors.append(f\"Missing administrative_info field: {key}\")\n",
        "            else:\n",
        "                errors.append(\"administrative_info must be an object\")\n",
        "\n",
        "        return errors\n",
        "\n",
        "    def analyze_proposal(self, proposal_json: str, model: str = \"claude-3-5-sonnet-20241022\") -> str:\n",
        "        \"\"\"\n",
        "        Analyzes a grant proposal using Claude API and returns structured feedback.\n",
        "\n",
        "        Args:\n",
        "            proposal_json: JSON string containing the proposal data\n",
        "            model: Claude model to use (default: claude-3-5-sonnet-20241022)\n",
        "\n",
        "        Returns:\n",
        "            Markdown-formatted review\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Parse and validate JSON\n",
        "            proposal_data = json.loads(proposal_json)\n",
        "            validation_errors = self.validate_json_structure(proposal_data)\n",
        "\n",
        "            if validation_errors:\n",
        "                return \"**Validation Errors:**\\n\" + \"\\n\".join(f\"- {error}\" for error in validation_errors)\n",
        "\n",
        "            # Prepare the user message\n",
        "            user_message = f\"\"\"Please review the following grant proposal JSON data and provide a comprehensive critique following the format of NIH summary statements:\n",
        "\n",
        "```json\n",
        "{proposal_json}\n",
        "```\n",
        "\n",
        "Provide your review following the exact format specified in your instructions, with particular attention to:\n",
        "1. Overall impact and significance\n",
        "2. Detailed strengths and weaknesses\n",
        "3. Specific recommendations for improvement\n",
        "4. Assessment of feasibility and innovation\n",
        "\n",
        "Make your critique constructive and actionable, similar to how expert reviewers provide feedback in NIH summary statements.\"\"\"\n",
        "\n",
        "            # Call Claude API with better error handling\n",
        "            try:\n",
        "                response = self.client.messages.create(\n",
        "                    model=model,\n",
        "                    max_tokens=4000,\n",
        "                    temperature=0.1,\n",
        "                    system=self.system_prompt,\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": user_message\n",
        "                        }\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                return response.content[0].text\n",
        "\n",
        "            except anthropic.RateLimitError:\n",
        "                return \"**Rate Limit Error:** Please wait a moment before trying again.\"\n",
        "            except anthropic.APIError as e:\n",
        "                return f\"**Anthropic API Error:** {str(e)}\"\n",
        "            except Exception as e:\n",
        "                return f\"**Unexpected API Error:** {str(e)}\"\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            return f\"**JSON Parsing Error:** {str(e)}\\n\\nPlease ensure your input is valid JSON.\"\n",
        "        except Exception as e:\n",
        "            return f\"**Error:** {str(e)}\"\n",
        "\n",
        "    def compare_with_summary_statement(self, proposal_json: str, summary_statement_json: str, model: str = \"claude-3-5-sonnet-20241022\") -> str:\n",
        "        \"\"\"\n",
        "        Compares the proposal with an existing summary statement to provide enhanced feedback.\n",
        "\n",
        "        Args:\n",
        "            proposal_json: JSON string containing the proposal data\n",
        "            summary_statement_json: JSON string containing the summary statement data\n",
        "            model: Claude model to use\n",
        "\n",
        "        Returns:\n",
        "            Comparative analysis in markdown format\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Parse JSONs\n",
        "            proposal_data = json.loads(proposal_json)\n",
        "            summary_data = json.loads(summary_statement_json)\n",
        "\n",
        "            # Prepare comparative analysis message\n",
        "            user_message = f\"\"\"Please analyze this grant proposal and compare it with the provided summary statement to give enhanced feedback:\n",
        "\n",
        "**ORIGINAL PROPOSAL:**\n",
        "```json\n",
        "{proposal_json}\n",
        "```\n",
        "\n",
        "**SUMMARY STATEMENT (for reference):**\n",
        "```json\n",
        "{summary_statement_json}\n",
        "```\n",
        "\n",
        "Provide a comprehensive analysis that:\n",
        "1. Reviews the proposal using the same standards as the summary statement\n",
        "2. Identifies how the proposal could be improved based on the feedback patterns in the summary statement\n",
        "3. Provides specific recommendations for strengthening weak areas\n",
        "4. Suggests how to maintain and enhance the strong aspects\n",
        "\n",
        "Format your response according to the structured format specified in your instructions.\"\"\"\n",
        "\n",
        "            # Call Claude API\n",
        "            try:\n",
        "                response = self.client.messages.create(\n",
        "                    model=model,\n",
        "                    max_tokens=4000,\n",
        "                    temperature=0.1,\n",
        "                    system=self.system_prompt,\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": user_message\n",
        "                        }\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "                return response.content[0].text\n",
        "\n",
        "            except anthropic.RateLimitError:\n",
        "                return \"**Rate Limit Error:** Please wait a moment before trying again.\"\n",
        "            except anthropic.APIError as e:\n",
        "                return f\"**Anthropic API Error:** {str(e)}\"\n",
        "            except Exception as e:\n",
        "                return f\"**Unexpected API Error:** {str(e)}\"\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            return f\"**JSON Parsing Error:** {str(e)}\\n\\nPlease ensure both inputs are valid JSON.\"\n",
        "        except Exception as e:\n",
        "            return f\"**Error:** {str(e)}\"\n",
        "\n",
        "# Convenience functions for Google Colab usage\n",
        "def setup_claude_reviewer(api_key: str = None):\n",
        "    try:\n",
        "        reviewer = ClaudeGrantReviewer(api_key)\n",
        "        return reviewer\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Get your API key from: https://console.anthropic.com/\")\n",
        "        return None\n",
        "\n",
        "def load_json_file(file_path: str) -> str:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "    json.loads(content)  # Validate JSON\n",
        "    return content\n",
        "\n",
        "def review_grant_proposal_from_file(reviewer: ClaudeGrantReviewer, proposal_file_path: str, display_review: bool = True):\n",
        "    if reviewer is None:\n",
        "        return None\n",
        "\n",
        "    proposal_json = load_json_file(proposal_file_path)\n",
        "    review = reviewer.analyze_proposal(proposal_json)\n",
        "\n",
        "    if display_review:\n",
        "        display(Markdown(review))\n",
        "\n",
        "    return review\n",
        "\n",
        "def compare_proposal_with_summary_from_files(reviewer: ClaudeGrantReviewer, proposal_file_path: str, summary_file_path: str, display_review: bool = True):\n",
        "    if reviewer is None:\n",
        "        return None\n",
        "\n",
        "    proposal_json = load_json_file(proposal_file_path)\n",
        "    summary_statement_json = load_json_file(summary_file_path)\n",
        "\n",
        "    analysis = reviewer.compare_with_summary_statement(proposal_json, summary_statement_json)\n",
        "\n",
        "    if display_review:\n",
        "        display(Markdown(analysis))\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Legacy functions (kept for backward compatibility)\n",
        "def review_grant_proposal(reviewer: ClaudeGrantReviewer, proposal_json: str, display_review: bool = True):\n",
        "    if reviewer is None:\n",
        "        return None\n",
        "\n",
        "    review = reviewer.analyze_proposal(proposal_json)\n",
        "\n",
        "    if display_review:\n",
        "        display(Markdown(review))\n",
        "\n",
        "    return review\n",
        "\n",
        "def compare_proposal_with_summary(reviewer: ClaudeGrantReviewer, proposal_json: str, summary_statement_json: str, display_review: bool = True):\n",
        "    if reviewer is None:\n",
        "        return None\n",
        "\n",
        "    analysis = reviewer.compare_with_summary_statement(proposal_json, summary_statement_json)\n",
        "\n",
        "    if display_review:\n",
        "        display(Markdown(analysis))\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Test with a simple, valid JSON example\n",
        "\n",
        "\n",
        "print(\"Claude Grant Reviewer Ready\")\n",
        "print(\"Usage:\")\n",
        "reviewer = setup_claude_reviewer()\n",
        "review = review_grant_proposal_from_file(reviewer, '/content/extracted_nsf_grant1.json')\n",
        "analysis = compare_proposal_with_summary_from_files(reviewer, '/content/extracted_nsf_grant1.json', '/content/nih_summary_extracted5.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2MGrJOgt4ds5",
        "outputId": "3b36a603-cdc6-4af4-c876-ec99cd005e63"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claude Grant Reviewer Ready\n",
            "Usage:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Validation Errors:**\n- Missing required section: project_description"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Grant Review Analysis\n\n**SUMMARY**\nThis proposal aims to develop an integrated platform combining Surface Plasmon Resonance (SPR) with computational modeling to characterize antibody:antigen binding, specifically focusing on HSV glycoprotein D. The project leverages high-throughput experimental methods and computational approaches to map epitopes and understand antibody recognition patterns, with potential applications in therapeutic antibody development and viral immunology.\n\n**OVERALL IMPACT AND SIGNIFICANCE**\n- The proposal presents an innovative technical approach to a significant problem in antibody characterization and viral immunology\n- The integration of computational and experimental methods shows strong potential for advancing the field\n- The focus on HSV glycoprotein D provides immediate clinical relevance while demonstrating broader applicability\n- However, like the reference summary statement's emphasis on preliminary data, this proposal would benefit from stronger validation data\n\n**STRENGTHS**\n- Strong multi-disciplinary team with complementary expertise across institutions\n- Innovative integration of computational and experimental approaches\n- Clear technical feasibility given the team's access to specialized equipment (Wasatch SPRi)\n- Well-defined methodology with specific, measurable objectives\n- Direct clinical relevance with broader implications for antibody characterization\n\n**WEAKNESSES AND AREAS FOR IMPROVEMENT**\n1. Preliminary Data Gap\n   - Issue: Limited preliminary data demonstrating feasibility\n   - Solution: Include pilot data showing successful SPR measurements with a subset of antibodies\n\n2. Validation Strategy\n   - Issue: Insufficient detail on validation approaches\n   - Solution: Add orthogonal validation methods (e.g., crystallography or cryo-EM)\n\n3. Risk Mitigation\n   - Issue: Limited discussion of potential technical challenges\n   - Solution: Include alternative approaches and contingency plans\n\n**DETAILED CRITIQUE BY CATEGORY**\n\n*Significance and Innovation:*\n- High potential impact on antibody characterization methodology\n- Novel integration of computational and experimental approaches\n- Could benefit from stronger emphasis on immediate clinical applications\n- Innovation level is high but needs better contextualization within current field\n\n*Approach and Methodology:*\n- Well-structured experimental design\n- Clear technical feasibility\n- Needs more detail on computational methods\n- Should include more validation steps\n\n*Team and Resources:*\n- Strong multi-institutional collaboration\n- Excellent mix of expertise\n- Access to necessary specialized equipment\n- Could benefit from additional computational biology expertise\n\n*Budget and Timeline:*\n- Budget details are incomplete in provided information\n- Timeline appears feasible but needs more detailed milestones\n- Equipment costs seem appropriate given the technical requirements\n\n**RECOMMENDATIONS**\n\n1. Priority Actions:\n   - Add preliminary data showing proof-of-concept results\n   - Develop more detailed validation strategy\n   - Strengthen computational methods description\n\n2. Enhancement Opportunities:\n   - Include more specific milestones and deliverables\n   - Add success criteria for each objective\n   - Expand risk mitigation strategies\n\n3. Impact Enhancement:\n   - Better articulate immediate clinical applications\n   - Strengthen connection to therapeutic development\n   - Add broader impact statement for non-HSV applications\n\n4. Learning from Summary Statement:\n   - Like the reference case, emphasize preliminary data\n   - Include more mechanistic studies\n   - Add clear validation approaches\n   - Strengthen methodology details\n\n5. Team Development:\n   - Consider adding computational biology collaborator\n   - Define clear roles for each team member\n   - Include regular team meeting schedule and coordination plan\n\nThe proposal shows strong potential but would benefit from addressing these recommendations to achieve the high standard of successful grants like the reference summary statement."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rrYJl32a5l5T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}